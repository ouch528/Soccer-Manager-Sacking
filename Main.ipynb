{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc28b6a-8343-4070-9df3-36483ba0ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import concurrent.futures\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58309c9e-94e8-4819-8810-c2c808205d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client_id = os.getenv('REDDIT_CLIENT_ID')\n",
    "client_secret = os.getenv('REDDIT_CLIENT_SECRET')\n",
    "user_agent = os.getenv('REDDIT_USER_AGENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b6e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up S3 client with explicit credentials\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.getenv('AWS_REGION')  # Replace with your region if different\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87691a7-caa4-4714-a75c-5b911dd07633",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id= client_id,\n",
    "    client_secret= client_secret,\n",
    "    user_agent= user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74c1bbd-5fd9-4e6e-b591-d4a0d49bc80f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to S3 successfully. Time taken: 11.76 seconds.\n"
     ]
    }
   ],
   "source": [
    "def fetch_comments(submission):\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    return [\n",
    "        {\n",
    "            'comment': comment.body,\n",
    "            'score': comment.score,\n",
    "            'submission_title': submission.title,\n",
    "        }\n",
    "        for comment in submission.comments.list()\n",
    "    ]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "subreddit_name = \"soccer\"\n",
    "search_query = \"Manchester United\"\n",
    "comments_data = []\n",
    "\n",
    "# Increase the number of workers in the ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Use list comprehension to create a list of futures\n",
    "    futures = [executor.submit(fetch_comments, submission) for submission in reddit.subreddit(subreddit_name).search(search_query, sort='new', limit=50)]\n",
    "    \n",
    "    # Use as_completed to process results as they become available\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        comments_data.extend(future.result())\n",
    "\n",
    "# Limit to the first 100 comments\n",
    "comments_data = comments_data[:100]\n",
    "\n",
    "# Convert to DataFrame\n",
    "comments_df = pd.DataFrame(comments_data)\n",
    "csv_file_path = 'manchester_united_comments.csv'\n",
    "\n",
    "# Save the comments DataFrame to CSV\n",
    "comments_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Upload the CSV file to S3\n",
    "s3_bucket_name = 'reddit-football-text'\n",
    "s3_file_name = 'man_utd.csv'\n",
    "\n",
    "s3.upload_file(csv_file_path, s3_bucket_name, s3_file_name)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Data uploaded to S3 successfully. Time taken: {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528ada51-aa6a-414d-9826-35dd4b699d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohungchan/Documents/GitHub/Soccer-Manager-Sacking/sagemaker-venv-3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained sentiment-analysis pipeline\n",
    "sentiment_pipeline = pipeline('sentiment-analysis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-venv-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
